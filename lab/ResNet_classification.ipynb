{"cells": [{"cell_type": "markdown", "metadata": {"id": "2qCfc4jH2UgQ", "tags": []}, "source": "# ResNet\nResNet stands for Residual Network. It's a type of convolutional neural network (CNN) architecture that was introduced in 2015 by researchers at Microsoft, and it won the [ImageNet](https://en.wikipedia.org/wiki/ImageNet) competition that year. If you are interested, you can read the original paper here: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385).  \n\nWhen you make a CNN deeper (by stacking more layers), you expect it to perform better because deeper networks can learn more complex features. However, in practice it could in fact become worse. ResNet's main innovation is the use of residual connections, a clever trick to address this problem.\n\nInstead of learning the direct mapping $y = \\mathcal{H}(x)$ as in fully-connected networks, ResNets learns what has to be added to the input to get the output. That is, the **residual** $\\mathcal{F}(x)$ where\n\n\\begin{align}\n\\mathcal{F}(x) =& \\;\\mathcal{H}(x) - x \\\\\n\\mathcal{F}(x) + x =&\\; \\mathcal{H}(x)\n\\end{align}\n\nIn practice, this is achieved through residual connections which acts as \"short-cuts\" for the input $x$. A residual block is defined as;\n\n![ResNet Basic Block](data:image/png;base64,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)\n\nwhere the weight layers can be convolutional layers. As you can see, the input $x$ skips the weight layers and get added to the residual $\\mathcal{F}(x)$ at the end of the block. A full ResNet model is built by sequentially stacking these blocks one after another.\n\nAuthors: Stina Brunzell, Albin \u00c5berg Dahlberg, Paul H\u00e4usner <br>\nLast update: 05.12.2024\n"}, {"cell_type": "markdown", "metadata": {"id": "KULebADpEgsq", "tags": []}, "source": "# Real world image classification\n\nThis notebook classifies real world images using ResNet-18 pretrained on [ImageNet](https://en.wikipedia.org/wiki/ImageNet). The integer 18 indicates that this ResNet has 18 layers:\n\n- 1 initial convolutional layer.\n- Each residual block consists of 2 convolutional layers. There are 8 sequential residual blocks in the model.\n- The final layer is a fully connected layer generating class predictions.\n\n$$1 + 2\\cdot 8 + 1 = 18$$"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "YZ-Fzv0WnOMY", "tags": []}, "outputs": [], "source": "# Import libraries\n%matplotlib inline\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport timm\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\nimport json\nfrom urllib.request import urlopen\nfrom urllib.error import URLError"}, {"cell_type": "markdown", "metadata": {"id": "mYz8s5ZwzIww", "tags": []}, "source": "We start by loading the model from a library called timm, short for PyTorch Image Models."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 176, "referenced_widgets": ["718af95c0cee4cadbd7479ffae103368", "55d16f02d9bb4a3b8338d825b943df03", "fcfd50f1b0af459980c52e4c94837d8e", "9dca98f22eb144d8bdd2da06d80e85e4", "aceb4d70b29944e0b841cc1860898532", "4e1b88b22fef4f51a5ecd012356a2927", "2b2462925d8e44aebdb4023260cc7c42", "a24d59a24bcb4d8d899e3aa7338496ad", "05041746da5b4e5393b123c84d52ba0f", "8ad06c7555a2478fa9a1cc06a2e2c712", "08e5a94374554c3ca962a45fd9476853"]}, "id": "jHkhTrHLnaqL", "outputId": "0d919d58-8bf6-4450-e0eb-3c8c4c508b70", "tags": []}, "outputs": [], "source": "resnet = timm.create_model(model_name='resnet18', pretrained=True)"}, {"cell_type": "markdown", "metadata": {"id": "JUUAh7R7z8Ho", "tags": []}, "source": "We switch the network to evaluation mode (that disables dropout and other features specific for training) and print a summary of the network architecture."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "collapsed": true, "id": "Rc_ZY8q3nOMa", "jupyter": {"outputs_hidden": true}, "outputId": "88e92efd-e80a-4855-b0da-bd876a9553fa", "tags": []}, "outputs": [], "source": "resnet.eval()"}, {"cell_type": "markdown", "metadata": {"id": "OMcmAo5-nOMa", "tags": []}, "source": "The pretrained ResNet model [expects](https://pytorch.org/docs/master/torchvision/models.html#classification)\n>  input images normalized in the same way as during training, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`.\n\nThus we define the following set of preprocessing steps, that resizes an input image, crops it to the requested size, converts the pixels to values in [0, 1], and then normalizes them accordingly."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "IgYv_3AmnOMa", "tags": []}, "outputs": [], "source": "preprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])"}, {"cell_type": "markdown", "metadata": {"id": "uPADM62VnOMb", "tags": []}, "source": "To be able to analyze the predictions of the model, we load [curated list of human-readable labels](https://github.com/anishathalye/imagenet-simple-labels) for the 1000 classes of the ImageNet dataset with which the model has been trained."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "UOBtwyaUnOMb", "scrolled": true, "tags": []}, "outputs": [], "source": "with urlopen('https://git.io/JvBFb') as f:\n    LABELS = json.load(f)"}, {"cell_type": "markdown", "metadata": {"id": "qaBtBVkizxrF", "tags": []}, "source": "We load an image that we want to classify, either from a local folder or the internet."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 424}, "id": "wOCvDY1qnOMb", "outputId": "042ffa92-609e-4763-bca1-53246abef915", "tags": []}, "outputs": [], "source": "# URL of the image that we want to analyze\n# prepend 'file:' to local paths (e.g., use 'file:./gorilla.jpeg')\nimage_url = 'https://w.wiki/HaK'\n\n# some examples from Wikipedia:\n# Gorilla https://w.wiki/HaK\n# Uppsala cathedral https://w.wiki/HaV\n# African bush elephant https://w.wiki/HaH\n# Pelle Svansl\u00f6s https://w.wiki/HaG\n# Hedgehog https://w.wiki/HaJ\n# Suspension bridge https://w.wiki/HaQ\n\ntry:\n    with Image.open(urlopen(image_url)) as im:\n        # The following fixes some problems when loading images:\n        # https://stackoverflow.com/a/64598016\n        image = im.convert(\"RGB\")\nexcept (URLError, OSError):\n    print(\"please provide a valid URL or local path\")\nelse:\n    print(f\"{image.mode} image of size {image.size}\")\n    plt.imshow(np.asarray(image))\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()"}, {"cell_type": "markdown", "metadata": {"id": "Vij14obPnOMb", "tags": []}, "source": "We preprocess the image, retrieve the predicted probabilities from the ResNet model."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "5xYO8WYDnOMb", "tags": []}, "outputs": [], "source": "# perform the pre-processing and form a \"batch\" of one single image\nX = preprocess(image).unsqueeze(0)\n\n# obtain the predicted probabilities for the image\nwith torch.no_grad():\n    G = F.softmax(resnet(X), dim=1)[0]"}, {"cell_type": "markdown", "metadata": {"id": "p_hYu9VCnOMc", "tags": []}, "source": "We plot the image and print the human-readable labels of the top 5 predictions."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 495}, "id": "Mw01MGzTnOMc", "outputId": "a6eaa6dc-8c67-4be7-9c95-fdf0f155e672", "tags": []}, "outputs": [], "source": "plt.imshow(image)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\nfor (p, y) in zip(*(G.topk(5))):\n    print(f\"{LABELS[y.item()]} ({100 * p.item():.2f}%)\")"}, {"cell_type": "markdown", "metadata": {"id": "kw8AjXxuruag", "tags": []}, "source": "# Experiment with other ResNets\nTry out other ResNet models (by changing `model_name` when loading the model) on a bunch of different images of your choice. Some exapmles of other ResNets are `resnet34`, `resnet50` and `resnet101`. Remember that the integer indicates the number of layers in the model.\n\nWhat are your findings?"}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "anaconda-cloud": {}, "celltoolbar": "Tags", "colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.7"}, "widgets": {"application/vnd.jupyter.widget-state+json": {"05041746da5b4e5393b123c84d52ba0f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "08e5a94374554c3ca962a45fd9476853": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2b2462925d8e44aebdb4023260cc7c42": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "4e1b88b22fef4f51a5ecd012356a2927": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "55d16f02d9bb4a3b8338d825b943df03": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_4e1b88b22fef4f51a5ecd012356a2927", "placeholder": "\u200b", "style": "IPY_MODEL_2b2462925d8e44aebdb4023260cc7c42", "value": "model.safetensors:\u2007100%"}}, "718af95c0cee4cadbd7479ffae103368": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_55d16f02d9bb4a3b8338d825b943df03", "IPY_MODEL_fcfd50f1b0af459980c52e4c94837d8e", "IPY_MODEL_9dca98f22eb144d8bdd2da06d80e85e4"], "layout": "IPY_MODEL_aceb4d70b29944e0b841cc1860898532"}}, "8ad06c7555a2478fa9a1cc06a2e2c712": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9dca98f22eb144d8bdd2da06d80e85e4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_8ad06c7555a2478fa9a1cc06a2e2c712", "placeholder": "\u200b", "style": "IPY_MODEL_08e5a94374554c3ca962a45fd9476853", "value": "\u200746.8M/46.8M\u2007[00:00&lt;00:00,\u2007111MB/s]"}}, "a24d59a24bcb4d8d899e3aa7338496ad": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "aceb4d70b29944e0b841cc1860898532": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fcfd50f1b0af459980c52e4c94837d8e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a24d59a24bcb4d8d899e3aa7338496ad", "max": 46807446, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_05041746da5b4e5393b123c84d52ba0f", "value": 46807446}}}}}, "nbformat": 4, "nbformat_minor": 4}