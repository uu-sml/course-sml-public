{"cells": [{"cell_type": "markdown", "metadata": {"tags": []}, "source": "# Notebook: F3 -- Logistic Regression\n\n*Authors*: Amir Baghi, Daniel Gedon<br>\n*Date*: 31.10.2023<br>\n*Updated*: 05.11.2024\n\nThis notebook is complementary to lecture F3 about Logistic Regression in order to highlight the key concepts. The focus will be on\n1. Understanding and visualizing different loss functions: **Misclassification** and **Logistic Loss**\n2. A basic classifier and its **Misclassification Loss** and modifying the parameters to see the effects on the loss.\n3. Finally, the same classifier with its **Logistic Loss** and visualizing the loss surface.\n\nPlease read the instructions and play around with the notebook where it is described."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "pip install -q ipywidgets"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# imports necessary libraries\n%matplotlib inline\n\nimport scipy.stats\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom ipywidgets import interact, widgets\n\nnp.random.seed(42) # fix the random seed"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "---\n\n## 1. Loss Functions: Misclassification and Logistic Loss\n\nIn this section we will look at two loss functions: the misclassification loss and the logistic loss.\n\n\nThe **misclassification loss** is defined as\n\n$$\nL_{\\text{misclass}}(y, \\hat{y}(x;\\theta)) = \\begin{cases}\n0 & y \\cdot \\hat{y}(x;\\theta) \\ge 0 \\\\\n1 & y \\cdot \\hat{y}(x;\\theta) \\lt 0\n\\end{cases}\n$$\n\nwhere $y$ is the true label for input $x$ in $\\{-1, 1\\}$ and $\\hat{y}(x;\\theta)$ is the predicted label from our machine learning model. This loss is very intuitive: it is 1 if the prediction and the true value don't match and it's 0 otherwise.\n\nMoreover, for the logistic regression case, where the decision boundary is linear given by $\\theta^T x$, the **logistic loss** is defined as\n\n$$\nL_{\\text{logistic}}(y, x;\\theta) = \\ln(1 + \\exp(-y \\cdot \\theta^T x)).\n$$\n\nThe logistic loss is basically a *continuous* approximation to the misclassification loss, taking into account also **how far away** our predictions are from the real labels.\n\nBelow, we have some helper functions for visualizing each of these loss functions. Skip over and go to the next box."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "def plot_misclassification_loss():\n    yyhat = np.linspace(-5, 5, 100)\n    loss = np.where(yyhat < 0, 1, 0)\n    plt.figure(figsize=(5, 3))\n    plt.plot(yyhat, loss)\n    plt.xlabel('$y \\cdot \\hat{y}(x)$')\n    plt.ylabel('Misclassification Loss')\n    plt.show()\n\ndef plot_logistic_loss():\n    yyhat = np.linspace(-5, 5, 100)\n    loss = np.log(1 + np.exp(-yyhat))\n    plt.figure(figsize=(5, 3))\n    plt.plot(yyhat, loss, c='orange') # try also with semilogy\n    plt.xlabel('$y \\cdot \\hat{y}(x)$')\n    plt.ylabel('Logistic Loss')\n    plt.show()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "Below, we visualize each of the loss functions in a 2D plot. The x-axis is the product of the real label $y$ and the predicted value $\\hat{y}(x; \\theta)$, and the y-axis is the loss $L(y, \\hat{y}(x; \\theta))$."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Plot the misclassification loss\nplot_misclassification_loss()\n\n# Plot the logistic loss\nplot_logistic_loss()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "---\n\n## 2. A basic classifier + the Misclassification Loss\n\nIn this section, we will look at a basic classifier and its misclassification loss. We will also modify the parameters to see the effects on the loss and the decision boundary.\n\nWe model our basic classifier as follows:\n\n$$\n\\hat{y}(x_1, x_2;\\theta_1, \\theta_2) = \\text{sign}(\\theta_1 x_1 + \\theta_2 x_2)\n$$\n\nwhere $\\theta_1$ and $\\theta_2$ are the weights. If we consider $\\theta= [\\theta_1, \\theta_2]$, then we can rewrite the above equation in a more compact form:\n\n$$\n\\hat{y}(x;\\theta) = \\text{sign}(\\theta^T x)\n$$\n\nUsing this model, we can compute the average misclassification loss given a set of parameters $\\theta$. This will be our cost function:\n\n$$\nJ_{\\text{misclass}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N L_{\\text{misclass}}(y_i, \\hat{y}(x_i;\\theta))\n$$\n\nwhere $N$ is the number of samples in the dataset.\n\nBelow we generate our dataset and there are some helper functions to visualize the decision boundary and calculate the misclassification loss. Skip over and go to the next box."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# generate synthetic dataset\nx = np.random.rand(100, 2) * 4\ny = np.where(x[:, 1] > x[:, 0], 1, -1)\n\ndef calculate_misclassification_cost(x, y, theta):\n    return np.sum(np.where(y * np.dot(x, theta) < 0, 1, 0)) / len(y)\n\ndef plot_decision_boundary(x, y, theta):\n    plt.figure(figsize=(5, 3))\n    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm.coolwarm)\n    x1 = np.linspace(0, 4, 100)\n    x2 = -theta[0] / theta[1] * x1\n    plt.plot(x1 , x2, c='black')\n\n    mesh = np.meshgrid(np.linspace(0, 4, 100),\n                       np.linspace(0, 4, 100))\n\n    Z = np.sign(np.dot(np.c_[mesh[0].ravel(), mesh[1].ravel()], theta))\n    Z = Z.reshape(mesh[0].shape)\n    plt.pcolormesh(mesh[0], mesh[1], Z, cmap=cm.coolwarm, alpha=0.2)\n\n    plt.xlim([0, 4])\n    plt.ylim([0, 4])\n    plt.xlabel('$x_1$')\n    plt.ylabel('$x_2$')\n    plt.show()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "Below, we plot the decision boundary for a classifier with the initialized $\\theta$ parameters, alongside our data points which are colored according to their true label. Moreover, the misclassification cost is also calculated for the classifier and printed.\n\nTasks:\n1. Play around with the parameters $\\theta_1$ and $\\theta_2$ to: \n    - Observe how the decision boundary changes. \n    - Observe how the misclassification cost changes. \n2. Try to minimize the cost by changing $\\theta_1$ and $\\theta_2$ in order to separate the data points as best as possible."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Our initial weight vector, i.e. theta1 and theta2\ntheta  = [1, -0.3]\n\n# Plot the decision boundary\nplot_decision_boundary(x, y, theta)\n\n# Calculate the misclassification cost\nprint(\"The misclassification rate: \", calculate_misclassification_cost(x, y, theta))"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "---\n\n## 2. The same classifier + the Logistic Loss\n\nIn this section, we will look at the same classifier as before, but this time we will use the logistic loss instead of the misclassification loss. We will also visualize the loss surface in addition to the decision boundary.\n\nRemembering our definition of the logistic loss, we can compute the average logistic loss given a set of parameters $\\theta$. This will be our cost function:\n\n\\begin{align}\nJ_{\\text{logistic}}(\\theta) &= \\frac{1}{N} \\sum_{i=1}^N L_{\\text{logistic}}(y_i, x_i; \\theta) \\\\\n                            &= \\frac{1}{N} \\sum_{i=1}^N \\ln(1 + \\exp(-y_i \\cdot \\theta^T x_i))\n\\end{align}\n\nwhere $N$ is the number of samples in the dataset.\n\nBelow are some helper functions to calculate and visualize the logistic loss function. Skip over and go to the next box."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "def calculate_logistic_cost(x, y, theta):\n    return np.sum(np.log(1 + np.exp(-y * np.dot(x, theta)))) / len(y)\n\ndef plot_logistic_loss(x, y, azimuth, elevation):\n    theta1 = np.linspace(-1, 1, 50)\n    theta2 = np.linspace(-1, 1, 50)\n    theta1, theta2 = np.meshgrid(theta1, theta2)\n    loss = np.zeros(theta1.shape)\n    for i in range(len(theta1)):\n        for j in range(len(theta2)):\n            theta = [theta1[i, j], theta2[i, j]]\n            loss[i, j] = calculate_logistic_cost(x, y, theta)\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(projection='3d')\n    ax.plot_surface(theta1, theta2, loss, cmap=cm.viridis)\n    ax.set_xlabel(r'$\\theta_1$')\n    ax.set_ylabel(r'$\\theta_2$')\n    ax.set_zlabel('Logistic Loss')\n    ax.view_init(elevation, azimuth)\n    ax.tick_params(axis='x', which='major', pad=3)\n    ax.tick_params(axis='y', which='major', pad=3)\n    ax.set_xticks(np.linspace(-1, 1, 5))\n    ax.set_yticks(np.linspace(-1, 1, 5))\n    plt.show()\n\ndef plot_log_loss_interactive(x, y):\n    interact(plot_logistic_loss, x=widgets.fixed(x), y=widgets.fixed(y), \n    azimuth=widgets.FloatSlider(min=0, max=360, step=10, value=0), \n    elevation=widgets.FloatSlider(min=0, max=90, step=10, value=20))"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "Below, we again draw the decision boundary of our classifier for the same dataset. However, this time we calculate and print the logistic loss instead of the misclassification loss. Moreover, the loss surface is also plotted, where you can see how the loss changes for different values of $\\theta_1$ and $\\theta_2$, for this specific dataset.\n\nTask:\n1. Try again to minimize the cost by changing $\\theta_1$ and $\\theta_2$ in order to separate the data points as best as possible. Note how the best decision boundary does not yield a cost of 0, but rather a small value now. What does this mean for the classifier?\n2. Inspect the loss surface and see how the loss changes for different values of $\\theta_1$ and $\\theta_2$. What parameters yield the lowest loss? Is it the same as the one you found?"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Our initial weight vector, i.e. theta1 and theta2\ntheta = [1, -0.3]\n\n# Plot the decision boundary\nplot_decision_boundary(x, y, theta)\nprint(\"The logistic loss: \", calculate_logistic_cost(x, y, theta))\n\n# Plot the logistic loss\nplot_log_loss_interactive(x, y)"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "---\n\n# Take-home message\n\n* Logistic regression is used for classification problems.\n* Logisitc regression is a linear model with a certain decision boundary.\n* We can use the misclassification loss or the logistic loss. The latter gives a better notion of the distance of a sample to the decision boundary.\n\n**Recommendation for further reading:** The material covered in this notebook is well-covered in the beginning of Chapter 3.2 in the course book."}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 2}