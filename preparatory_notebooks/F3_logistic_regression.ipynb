{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: F3 -- Logistic Regression\n",
    "\n",
    "This notebook is complementary to lecture F3 about Logistic Regression in order to highlight the key concepts. The focus will be on\n",
    "1. Understanding and visualizing different loss functions: **Misclassification** and **Logistic Loss**\n",
    "2. A basic classifier and its **Misclassification Loss** and modifying the parameters to see the effects on the loss.\n",
    "3. Finally, the same classifier with its **Logistic Loss** and visualizing the loss surface.\n",
    "\n",
    "Please read the instructions and play around with the notebook where it is described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necessary libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "np.random.seed(42) # fix the random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Loss Functions: Misclassification and Logistic Loss\n",
    "\n",
    "In this section we will look at two loss functions: the misclassification loss and the logistic loss.\n",
    "\n",
    "The **misclassification loss** is defined as\n",
    "\n",
    "$$\n",
    "\\ell_{\\text{misclass}}(y, \\hat{y}) = \\begin{cases}\n",
    "0 & y\\hat{y} \\ge 0 \\\\\n",
    "1 & y\\hat{y} \\lt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $y$ is the true label and $\\hat{y}$ is the predicted label.\n",
    "\n",
    "Moreover, the **logistic loss** is defined as\n",
    "\n",
    "$$\n",
    "\\ell_{\\text{logistic}}(y, \\hat{y}) = \\ln(1 + \\exp(-y\\hat{y}))\n",
    "$$\n",
    "\n",
    "where $y$ is the true label and $\\hat{y}$ is the predicted label. \n",
    "\n",
    "The logistic loss is basically a *continuous* approximation to the misclassification loss, taking into account also **how far away** our predictions are from the real labels.\n",
    "\n",
    "Below, we have some helper functions for visualizing each of these loss functions. Skip over and go to the next box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_misclassification_loss():\n",
    "    yyhat = np.linspace(-5, 5, 100)\n",
    "    loss = np.where(yyhat < 0, 1, 0)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(yyhat, loss)\n",
    "    plt.xlabel('$y \\cdot \\hat{y}$')\n",
    "    plt.ylabel('Misclassification Loss')\n",
    "    plt.show()\n",
    "\n",
    "def plot_logistic_loss():\n",
    "    yyhat = np.linspace(-5, 5, 100)\n",
    "    loss = np.log(1 + np.exp(-yyhat))\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(yyhat, loss, c='orange') # try also with semilogy\n",
    "    plt.xlabel('$y \\cdot \\hat{y}$')\n",
    "    plt.ylabel('Logistic Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we visualize each of the loss functions in a 2D plot. The x-axis is the product of the real value $y$ and the predicted one $\\hat{y}$, and the y-axis is the loss $\\ell(y, \\hat{y})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the misclassification loss\n",
    "plot_misclassification_loss()\n",
    "\n",
    "# Plot the logistic loss\n",
    "plot_logistic_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. A basic classifier + the Misclassification Loss\n",
    "\n",
    "In this section, we will look at a basic classifier and its misclassification loss. We will also modify the parameters to see the effects on the loss and the decision boundary.\n",
    "\n",
    "We model our basic classifier as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(\\theta_1 x_1 + \\theta_2 x_2)\n",
    "$$\n",
    "\n",
    "where $\\theta_1$ and $\\theta_2$ are the weights. If we consider $\\theta= [\\theta_1, \\theta_2]$, then we can rewrite the above equation in a more compact form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(\\theta^T x)\n",
    "$$\n",
    "\n",
    "Using this model, we can compute the average misclassification loss given a set of parameters $\\theta$. This will be our cost function:\n",
    "\n",
    "$$\n",
    "J_{\\text{misclass}}(w) = \\frac{1}{N} \\sum_{i=1}^N \\ell_{\\text{misclass}}(y_i, \\hat{y}_i ; \\theta)\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples in the dataset.\n",
    "\n",
    "Below we generate our dataset and there are some helper functions to visualize the decision boundary and calculate the misclassification loss. Skip over and go to the next box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic dataset\n",
    "x = np.random.rand(100, 2) * 4\n",
    "y = np.where(x[:, 1] > x[:, 0], 1, -1)\n",
    "\n",
    "def calculate_misclassification_cost(x, y, theta):\n",
    "    return np.sum(np.where(y * np.dot(x, theta) < 0, 1, 0)) / len(y)\n",
    "\n",
    "def plot_decision_boundary(x, y, theta):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm.coolwarm)\n",
    "    x1 = np.linspace(0, 4, 100)\n",
    "    x2 = -theta[0] / theta[1] * x1\n",
    "    plt.plot(x1 , x2, c='black')\n",
    "\n",
    "    mesh = np.meshgrid(np.linspace(0, 4, 100),\n",
    "                       np.linspace(0, 4, 100))\n",
    "\n",
    "    Z = np.sign(np.dot(np.c_[mesh[0].ravel(), mesh[1].ravel()], theta))\n",
    "    Z = Z.reshape(mesh[0].shape)\n",
    "    plt.pcolormesh(mesh[0], mesh[1], Z, cmap=cm.coolwarm, alpha=0.2)\n",
    "\n",
    "    plt.xlim([0, 4])\n",
    "    plt.ylim([0, 4])\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the decision boundary for a classifier with the initialized $w$ parameters, alongside our data points which are colored according to their true label. Moreover, the misclassification cost is also calculated for the classifier and printed.\n",
    "\n",
    "Tasks:\n",
    "1. Play around with the parameters $\\theta_1$ and $\\theta_2$ to: \n",
    "    - Observe how the decision boundary changes. \n",
    "    - Observe how the misclassification cost changes. \n",
    "2. Try to minimize the cost by changing $\\theta_1$ and $\\theta_2$ in order to separate the data points as best as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our initial weight vector, i.e. theta1 and theta2\n",
    "theta  = [1, -0.3]\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(x, y, theta)\n",
    "\n",
    "# Calculate the misclassification cost\n",
    "print(\"The misclassification rate: \", calculate_misclassification_cost(x, y, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The same classifier + the Logistic Loss\n",
    "\n",
    "In this section, we will look at the same classifier as before, but this time we will use the logistic loss instead of the misclassification loss. We will also visualize the loss surface in addition to the decision boundary.\n",
    "\n",
    "Remembering our definition of the logistic loss, we can compute the average logistic loss given a set of parameters $w$. This will be our cost function:\n",
    "\n",
    "$$\n",
    "J_{\\text{logistic}}(w) = \\frac{1}{N} \\sum_{i=1}^N \\ell_{\\text{logistic}}(y_i, \\hat{y}_i ; \\theta)\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples in the dataset.\n",
    "\n",
    "Below are some helper functions to calculate and visualize the logistic loss function. Skip over and go to the next box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_logistic_cost(x, y, theta):\n",
    "    return np.sum(np.log(1 + np.exp(-y * np.dot(x, theta)))) / len(y)\n",
    "\n",
    "def plot_logistic_loss(x, y, azimuth, elevation):\n",
    "    theta1 = np.linspace(-1, 1, 50)\n",
    "    theta2 = np.linspace(-1, 1, 50)\n",
    "    theta1, theta2 = np.meshgrid(theta1, theta2)\n",
    "    loss = np.zeros(theta1.shape)\n",
    "    for i in range(len(theta1)):\n",
    "        for j in range(len(theta2)):\n",
    "            theta = [theta1[i, j], theta2[i, j]]\n",
    "            loss[i, j] = calculate_logistic_cost(x, y, theta)\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.plot_surface(theta1, theta2, loss, cmap=cm.viridis)\n",
    "    ax.set_xlabel(r'$\\theta_1$')\n",
    "    ax.set_ylabel(r'$\\theta_2$')\n",
    "    ax.set_zlabel('Logistic Loss')\n",
    "    ax.view_init(elevation, azimuth)\n",
    "    ax.tick_params(axis='x', which='major', pad=3)\n",
    "    ax.tick_params(axis='y', which='major', pad=3)\n",
    "    ax.set_xticks(np.linspace(-1, 1, 5))\n",
    "    ax.set_yticks(np.linspace(-1, 1, 5))\n",
    "    plt.show()\n",
    "\n",
    "def plot_log_loss_interactive(x, y):\n",
    "    interact(plot_logistic_loss, x=widgets.fixed(x), y=widgets.fixed(y), \n",
    "    azimuth=widgets.FloatSlider(min=0, max=360, step=10, value=0), \n",
    "    elevation=widgets.FloatSlider(min=0, max=90, step=10, value=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we again draw the decision boundary of our classifier for the same dataset. However, this time we calculate and print the logistic loss instead of the misclassification loss. Moreover, the loss surface is also plotted, where you can see how the loss changes for different values of $w_1$ and $w_2$, for this specific dataset.\n",
    "\n",
    "Task:\n",
    "1. Try again to minimize the cost by changing $\\theta_1$ and $\\theta_2$ in order to separate the data points as best as possible. Note how the best decision boundary does not yield a cost of 0, but rather a small value now. What does this mean for the classifier?\n",
    "2. Inspect the loss surface and see how the loss changes for different values of $\\theta_1$ and $\\theta_2$. What parameters yield the lowest loss? Is it the same as the one you found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our initial weight vector, i.e. theta1 and theta2\n",
    "theta = [1, -0.3]\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(x, y, theta)\n",
    "print(\"The logistic loss: \", calculate_logistic_cost(x, y, theta))\n",
    "\n",
    "# Plot the logistic loss\n",
    "plot_log_loss_interactive(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Take-home message\n",
    "\n",
    "* Logistic regression is used for classification problems.\n",
    "* Logisitc regression is a linear model with a certain decision boundary.\n",
    "* We can use the misclassification loss or the logistic loss. The latter gives a better notion of the distance of a sample to the decision boundary.\n",
    "\n",
    "**Recommendation for further reading:** The material covered in this notebook is well-covered in the beginning of Chapter 3.2 in the course book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
