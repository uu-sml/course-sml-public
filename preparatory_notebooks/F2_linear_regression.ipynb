{"cells": [{"cell_type": "markdown", "metadata": {"id": "czDam0lOe-Xx", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "# Notebook: F2 -- Linear Regression\n\n*Authors*: Hanna Malmvall, Jennifer Andersson<br>\n*Date*: 31.10.2023\n\nThis notebook is complementary to lecture F2 about linear regression. The purpose of the notebook is to highlight key concepts. It is also an opportunity to refresh your knowledge and gain intuition. The focus will be on:\n1. **Generating data** for supervised machine learning problems\n2. **Fitting linear models** to this data\n3. **Evaluating** the fitted models to see how it performs on new data\n\nPlease read the instructions and play around with the notebook where it is described."}, {"cell_type": "markdown", "metadata": {"id": "okNobqIurFRS", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "---\n\nWe start by importing necessary libraries. These libraries will be used throughout the course. If you are unfamiliar with them or need to refresh your knowledge, we recommended to take a look at the \"Introduction to Python\" material available on Studium."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "C8P18Lggc55a", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split"}, {"cell_type": "markdown", "metadata": {"id": "Tn5lIINVc9bc", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "---\n\n## 1. Data Generation\n\nThe first step when solving a supervised machine learning problem is to **get a dataset**. Each input $x_i$ comes with a corresponding output or label $y_i$. Here, $i$ denotes the index of a particular sample, and we collect $n$ samples in total. Compactly, we denote our dataset as $\\mathcal{T} = \\{(x_i, y_i)\\}_{i=1}^{n}$.\n\nNow we:\n1. Generate a synthetic dataset $\\mathcal{T}$.\n2. Split the dataset into one train dataset and one test dataset. The train dataset will be used to fit a model to the data, and the test dataset will be used to evaluate our model.\n\nThe **goal** of our supervised machine learning method is to find a model that performs well on the unseen test data. Therefore, it is important to leave out a part of the data (the test dataset) from the training process to be able to evaluate how well our model will perform on new input datapoints $x$ in the future.\n\nBelow, we have some helper function to generate synthetic data, split the data into a train- and test dataset and then plot them. Run the cell and continue to the next cell."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "HHjHft5Iga81", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "# Generate synthetic data\ndef generate_synthetic_data():\n    np.random.seed(0)\n    X = np.random.rand(100, 1)  # Feature (independent variable)\n    y = np.e * X + np.pi/2 + np.random.normal(0, 0.1, (100,1)) # Target (dependent variable)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n    # Print table header\n    print(\"           | #x  | #y  |\")\n    print(\"-----------|-----|-----|\")\n    # Print train data row\n    print(f\"Train Data | {np.shape(X_train)[0]}{' ' * (3 - len(str(np.shape(X_train)[0])))} | {np.shape(y_train)[0]}{' ' * (3 - len(str(np.shape(y_train)[0])))} |\")\n    # Print test data row\n    print(f\"Test Data  | {np.shape(X_test)[0]}{' ' * (3 - len(str(np.shape(X_test)[0])))} | {np.shape(y_test)[0]}{' ' * (3 - len(str(np.shape(y_test)[0])))} |\")\n\n    return X_train, y_train, X_test, y_test\n\n# Plot the train data and test data\ndef plot_data():\n    plt.scatter(X_train, y_train, label='Training Data', alpha=0.5)\n    plt.scatter(X_test, y_test, label='Testing Data', alpha=0.5)\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()"}, {"cell_type": "markdown", "metadata": {"id": "W9oSQ59uhW7U", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "Now we can generate our datasets and plot them to get an understanding of what our data looks like. We plot both our train data (in blue) and our test data (in orange).\n\nTask:\n- Run the cell below to visualize the synthetic train- and test datasets.\n- Check if the test data is representative of the train data.\n- Is there some relationship between $x$ and $y$?"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 522}, "id": "AzuGzLTZfsrk", "outputId": "f2f83261-fb19-4d92-9e8f-d8aa3c64ea27", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "# generate data\nX_train,y_train, X_test, y_test = generate_synthetic_data()\n\n# plot the data\nplot_data()"}, {"cell_type": "markdown", "metadata": {"id": "HoYjDXdJPlwK", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "---\n\n## 2. Explore the Family of Linear Models\nFrom our plot, we notice that there seems to be a linear pattern: $y$ increases linearly with $x$. Hence, it might be suitable to use a **linear model** on the form:\n\n$$\ny=\u03b8_0+\u03b8_1x + \u03f5\n$$\n\nWe call $\u03b8_0$ and $\u03b8_1$ the **parameters** of our model. $\u03f5$ is a noise term capturing random errors in our data that our model does not account for.\n\nFinding a **good model** amount to fitting our model to the data. In other words, we want to find good values of $\u03b8_0$ and $\u03b8_1$, so that $y_i\\approx\u03b8_0+\u03b8_1x_i$ holds for the samples in our training dataset $\\mathcal{T}_{train} = \\{x_i, y_i\\}_{i=1}^{m}$. Here, $m$ denotes the number of samples in our train set, i.e. $m=80$.\n\nBelow is a helper function that plots some linear models. Skip over and go to the next box."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "CsVHfVtU_f5t", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "def plot_linear_models(\n    X, y, label='Training',\n    model1_params=[],\n    model2_params=[],\n    model3_params=[],\n):\n    # model 1\n    if not all(element is None for element in model1_params):\n        y_model1 = model1_params[0] + X * model1_params[1]\n        plt.plot(X, y_model1, 'r', label='Model 1', alpha=0.5)\n\n    # model 2\n    if not all(element is None for element in model2_params):\n        print('aaa')\n        y_model2 = model2_params[0] + X * model2_params[1]\n        plt.plot(X, y_model2, 'm', label='Model 2', alpha=0.5)\n\n    # model 3\n    if not all(element is None for element in model3_params):\n        y_model3 = model3_params[0] + X * model3_params[1]\n        plt.plot(X, y_model3, 'g', label='Model 3', alpha=0.5)\n\n    # Plot the training data\n    plt.scatter(X, y, label=label+' Data', alpha=0.5)\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()"}, {"cell_type": "markdown", "metadata": {"id": "LE75rcTF_f5t", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "In this section we want to find a good linear model. We plot the training data, as well as the linear models (which are fully described by $\u03b8_0$ and $\u03b8_1$). If a model fits the data, we can use our parameters along with the inputs of the data (variable $\\mathtt{X\\_train}$) to calculate predicted y-values which are close to the true y-values.\n\nTasks:\n\n1. Run the code below and visualize model 1 with the given parameters. Does it fit the data?\n2. Try to optimize the parameters of model 2 and model 3 to obtain better fits to the data. Replace the $\\mathtt{None}$ values with what you think are better parameters.\n3. Which set of parameters fits the data the best?\n4. What does $\u03b8_0$ and $\u03b8_1$ stand for?"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ssVhf5BD_f5u", "outputId": "a8c366f9-45c1-4e01-f26b-10a2b1c3f69c", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "# model 1:\ntheta0 = 3\ntheta1 = -4\nmodel1_params = [theta0, theta1]\n\n# model 2\ntheta0 = None\ntheta1 = None\nmodel2_params = [theta0, theta1]\n\n\n# model 3:\ntheta0 = None\ntheta1 = None\nmodel3_params = [theta0, theta1]\n\n# plot model fits\nplot_linear_models(X_train, y_train, 'Training',\n                   model1_params, model2_params, model3_params)"}, {"cell_type": "markdown", "metadata": {"id": "o7GqnLWy7dm0", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "---\n\n## 3. Model evaluation\n\nIn the above exercise, you visually fit the linear model to the data. But how can we quantitatively determine which model is better? A common metric is the mean squared error (MSE):\n\n$$\n\\frac{1}{m} \\sum_{i=1}^{m} {(y_i - f_{\\theta}(x_i))}^2\n$$\n\nHere, $y_i$ denotes the true value for each input $x_i$ in the train dataset, and $f_{\\theta}(x_i) = \\theta_0 + \\theta_{1}x_i$ is the output of the model parameterized by our particular choice of $\\theta_0$ and $\\theta_1$.\n\nBelow you can find two helper functions computing the model predictions for a particular model as well as the mean squared error of that model on the given data points. Skip over and go to the next box."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "hOl9HG_0_f5u", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "def model_prediction(X, model_params):\n    return model_params[0] + X * model_params[1]\n\ndef MSE(y, pred):\n    m = len(y)\n    return np.sum((y - pred)**2)/m"}, {"cell_type": "markdown", "metadata": {"id": "253dAe75_f5u", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "The following two code cells perform the following:\n- Compare the MSE of the three models on the **train dataset**.\n- Compare the MSE of the three models on the **test dataset** and plot the function with the test data.\n\nTasks:\n1. Do you want to minimize or maximise MSE?\n2. Given the train MSE, which model would you choose? Does this align with you visual impression from above?\n3. Does the model generalize to unseen test data? Or more specifically: Does the MSE on train data and test data match?"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "WNDAkUdSnaPz", "outputId": "45e34c69-b3ac-49ac-f925-25ee8b525e9c", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "print('MSE on train data')\n\npred1 = model_prediction(X_train, model1_params)\nmse1 = MSE(y_train, pred1)\nprint(f'Model 1: {mse1:.3f}')\n\npred2 = model_prediction(X_train, model2_params)\nmse2 = MSE(y_train, pred2)\nprint(f'Model 2: {mse2:.3f}')\n\npred3 = model_prediction(X_train, model3_params)\nmse3 = MSE(y_train, pred3)\nprint(f'Model 3: {mse3:.3f}')"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "HXPpgsGvpJbk", "outputId": "9f1fc9b4-3e25-41b3-8169-bd5afa899dd1", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "plot_linear_models(X_test, y_test, 'Test',\n                   model1_params, model2_params, model3_params)\n\nprint('MSE on test data')\n\npred1 = model_prediction(X_test, model1_params)\nmse1 = MSE(y_test, pred1)\nprint(f'Model 1: {mse1:.3f}')\n\npred2 = model_prediction(X_test, model2_params)\nmse2 = MSE(y_test, pred2)\nprint(f'Model 2: {mse2:.3f}')\n\npred3 = model_prediction(X_test, model3_params)\nmse3 = MSE(y_test, pred3)\nprint(f'Model 3: {mse3:.3f}')"}, {"cell_type": "markdown", "metadata": {"id": "ET6GtSDwrXGm", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "---\n\n## 4. Finding the \"optimal\" linear model:\n\nNow, as we have seen, we can not only fit a model by visual inspection but also select a model quantitatively by finding the model with the lowest MSE.\n\nBut is there a **systematic way** to select the model parameters $\\theta_0$ and $\\theta_1$? We define the \"best possible linear model\" as the model generating the smallest MSE. Finding the model parameters that minimize the MSE is equivalent to finding the parameters that minimize the squared L2-norm of the residual vector. Thus, to find the best linear model, we want to solve the following optimization problem with respect to $\\theta=[\\theta_0, \\theta_1]^\\top$:\n\n$$\n\\hat{\\mathbf{\\theta}} = \\text{arg}\\min_{\\mathbf{\\theta}} \\frac{1}{m} \\sum_{i=1}^{m} {(y_i - f_{\\theta}(x_i))}^2 = \\text{arg}\\min_{\\mathbf{\\theta}} ||{(\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta})}||_2^2\n$$\n\nwhere\n\\begin{align*}\n\\mathbf{y} &= \\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots\\\\\n    y_m\n\\end{bmatrix}\n& \\mathbf{X} &= \\begin{bmatrix}\n    1 & x_1 \\\\\n    1 & x_2 \\\\\n    \\vdots & \\vdots \\\\\n    1 & x_m\n\\end{bmatrix}\n& \\mathbf{\u03b8} &= \\begin{bmatrix}\n    \u03b8_0 \\\\\n    \u03b8_1 \\\\\n\\end{bmatrix}\n\\end{align*}\n\nWe use $\\hat{\\mathbf{\\theta}}$ to denote our estimates of the true parameters.The solution to this optimization problem finds the least squares solution $\\hat{\\mathbf{\\theta}}$ to the following (overdetermined) linear system of equations:\n\n$$\n\\mathbf{y}=\\mathbf{X}\\mathbf{\\theta}\n$$\n\n\nWe say that we find the solution that minimizes the **least squares cost**. So when we say that our model is the **optimal** linear model, we mean that it is optimal in a least squares sense given the data.\n\nWhen working with linear models, the optimization problem above has a closed-form solution that can be found by solving the normal equations for $\\hat{\\mathbf{\\theta}}$:\n\n$$\n\\mathbf{X}^T\\mathbf{X}\\hat{\\mathbf{\\theta}}=\\mathbf{X}^T\\mathbf{y}\n$$\n\nIn the cell below, we solve the normal equations using our train data:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "cVJhyY80Ie1K", "outputId": "88024254-82dd-4b5c-eec5-27c27a05c13f", "pycharm": {"name": "#%%\n"}, "tags": []}, "outputs": [], "source": "# construct the matrix X\nn = len(X_train) # number of samples in our training data\nX = np.ones((n, 2))\nX[:,1] = X_train[:,0]\n\n# solve the normal equations\ntheta_ls = np.linalg.inv(X.T@X)@(X.T@y_train)\nprint('Least Squares Solution:')\nprint(f'theta_0 = {theta_ls[0][0]}')\nprint(f'theta_1 = {theta_ls[1][0]}')"}, {"cell_type": "markdown", "metadata": {"id": "aFfFDLDZJ9o8", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "Task:\n1. Compare the optimal linear model with the best one that you found\n2. What is the train and test MSE of this optimal model?"}, {"cell_type": "markdown", "metadata": {"id": "gu5bHt0zMOa9", "pycharm": {"name": "#%% md\n"}, "tags": []}, "source": "---\n\n# Take-home message\n\n\n*   Collect a dataset and split the dataset into a train set (for training your model) and a test set (for evaluating your model).\n*   Define the model $f_{\\theta}(x)$ you want to fit to the data. In the case of linear regression, we let $f_{\\theta}(x)$ be the family of linear models parameterized by $\\theta$.\n* Choose an error metric and set up an optimization problem to find the optimal parameters $\\theta$. Here, we choose the MSE.\n* Solve the optimization problem using the train dataset.\n* Evaluate your model on the test dataset.\n\n**Recommendation for further reading:** The material covered in this notebook is well-covered in the beginning of Chapter 3.1 in the course book.\n\n\n"}], "metadata": {"celltoolbar": "Tags", "colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 1}