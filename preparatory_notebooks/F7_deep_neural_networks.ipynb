{"cells": [{"cell_type": "markdown", "metadata": {"id": "def99671", "tags": []}, "source": "# Notebook: F7 -- Deep Neural Networks\n\n*Authors*: Hugo Toll, Ziwei Luo<br>\n*Date*: Nov 2023\n\nThis notebook is complementary to lecture F7 about Deep Neural Networks in order to highlight some key concepts. The focus will be on\n1. Understanding the deep nerual network and different activation functions such as **ReLU**, **Sigmoid**, and **Tanh**.\n2. Exploring basic network structures in [NN playground](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.24826&showTestData=false&discretize=true&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&showTestData_hide=false).\n\n3. How a simple Neural Network can be implemented with PyTorch [*optional*].\n\nPlease read the instructions and play around with the notebook where it is described."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ccd682e7", "tags": []}, "outputs": [], "source": "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split"}, {"cell_type": "markdown", "metadata": {"id": "SMHylGVE-fmP", "tags": []}, "source": "---\n## 1. Composition of a Neural Network\nA cornerstone of the neural network model is linear regression. We can write the linear regression model in the following form:\n\n$$f(\\mathbf{x}; \\mathbf{\\theta}) = \\mathbf{W} \\mathbf{x} + \\mathbf{b} ,$$\n\nwhere $\\mathbf{x} = [x_1,..., x_d]^\\intercal$ and $\\mathbf{\\theta} = [\\mathbf{W}, \\,\\mathbf{b}]^\\intercal$.\n\nThe neural network for regression is built on this model, but with a twist! More specifically, a neural network contains multiple linear regressions with nonlinear activation functions $h(\\cdot)$, which can be written as\n$$f(\\mathbf{x}; \\mathbf{\\theta}) = \\mathbf{W}^{(2)\\intercal} \\mathbf{q(\\mathbf{x})} + \\mathbf{b}^{(2)}$$\n\n$$ \\mathbf{q}(\\mathbf{x}) = h(\\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)})\n$$\n\nwhere $\\mathbf{q}(\\mathbf{x})$ is an intermediate variable. Note that the matrices ($\\mathbf{W}^{(1)}$, $\\mathbf{W}^{(2)}$) and vectors ($\\mathbf{b}^{(1)}$, $\\mathbf{b}^{(2)}$) are trainable parameters of this model.\n"}, {"cell_type": "markdown", "metadata": {"id": "tEdW8uQQ-pgX", "tags": []}, "source": "### 1.1 Activation functions\nThe activation function is a non-linear function that is applied elementwise. One commonly used activation function in neural networks is the **Re**ctified **L**inear **U**nit (ReLU), which is defined as follows:\n$$\\mathrm{ReLU}(x) = \\mathrm{max}(x, 0).$$\n\nThe activation function is used to insert non-linearity into the network. Without a non-linear activation function, the model can be reduced to a simple linear regression, which renders the additional step of transforming the input variables with $\\mathbf{q}(\\mathbf{x})$ obsolete.\n\nIn addition, we can also use the **Sigmoid** and **Tanh** as activation functions that are defined by\n$$ \\mathrm{Sigmoid}(x) = \\frac{1}{1+e^{-x}}, \\quad \\mathrm{Tanh}(x) = \\frac{e^{x} - e^{-x}}{e^{x}+e^{-x}}. $$\n\n\nBelow is an example of fitting non-linear data with a two-layer neural network."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "kdiMlNJt-vzn", "tags": []}, "outputs": [], "source": "np.random.seed(0)\nX = 2*np.random.rand(100, 1)  # Feature (independent variable)\ny = np.e ** X + np.pi + np.random.normal(0, 0.2, (100,1)) # Target (dependent variable)\n\ndef linear(x, w, b):\n    out = np.dot(w, x) + b\n    return out\n\ndef relu(x):\n    out = x.copy()\n    out[out<0] = 0\n    return out\n\ndef sigmoid(x):\n    out = 1 / (1 + np.exp(-x))\n    return out\n\ndef tanh(x):\n    out = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n    return out\n\ndef plot_neural_network(data_X, data_y, actvation_fn,\n                        param_W1, param_b1,\n                        param_W2, param_b2):\n    q = linear(X, W1, b1)\n    q_activate = actvation_fn(q)\n    out = linear(q_activate, W2, b2)\n\n    plt.scatter(X, y, alpha=0.5, label=\"Training data\")\n    plt.scatter(X, q, alpha=0.5, label=r\"1st layer's output\")\n    plt.scatter(X, q_activate, alpha=0.5, label='After activation')\n    plt.scatter(X, out, alpha=0.5, label=\"2nd layer's output\")\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()"}, {"cell_type": "markdown", "metadata": {"id": "fTCtoK0-ALp0", "tags": []}, "source": "**Task:**\n\n- Can you change the parameter to other values and fit the same dataset again?"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "G_CrhKftALp2", "tags": []}, "outputs": [], "source": "W1, b1 = -1.2, 3\nW2, b2 = -19.9, 23\nact_fn = sigmoid\n\nplot_neural_network(X, y, act_fn, W1, b1, W2, b2) # don't need to be perfectly matched"}, {"cell_type": "markdown", "metadata": {"id": "zlY8BlYl-5Kb", "tags": []}, "source": "### 1.2 Neural Network with multiple hidden layers\nTo get a more complex neural network multiple transformations $\\mathbf{q}$ of the input data can be added in series, so that the result of one transformation is the input to the next transformation. This gives the following network structure:\n\n$$\\begin{align}\n\\mathbf{q}^{(0)} &= \\mathbf{x}\\\\\n\\mathbf{q}^{(l)} &= h(\\mathbf{W}^{(l)}\\mathbf{q}^{(l-1)} + \\mathbf{b}^{(l)}),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, l = 1,2,...,L-1 \\\\\nf(\\mathbf{x}; \\mathbf{\\theta}) &= \\mathbf{W}^{(L)\\intercal} \\mathbf{q}^{(L-1)} + \\mathbf{b}^{(L)}\\end{align}$$\n\nEach element in the vectors $\\mathbf{q}^{(l)}$ is called a node, neuron or hidden unit. And the network is said to have $L-1$ **hidden layers**, since the user of the model often only sees the input data and the result from the output layer."}, {"cell_type": "markdown", "metadata": {"id": "NVjBl8KQ-_xI", "tags": []}, "source": "### 1.3 Neural Network for classification\n#### Binary classification with logistic function\nNeural networks can be used for binary classification. This is achieved in the same way the linear regression model is extended for classification, we use the logistic function *in the last layer*! Notably, the logistic function is defined as the same as the *Sigmoid* function:\n$$f(x) = \\frac{1}{1+e^{-x}}.$$\n\nThis function transforms all values of $x$ into a range between 0 and 1, which means that we can interpret the output value as a probability. So to use a neural network for binary classification we can simply transform the output of the last layer with a logistic function."}, {"cell_type": "markdown", "metadata": {"id": "Q1iNXh8d_NLm", "tags": []}, "source": "#### Multiclass classification and softmax\n\nFor multiclass classification with $M$ output labels the output layer needs to have $M$ nodes. A special activation function called **softmax** is used *in the last layer* to transform the values of these nodes to probabilities. The softmax function is defined as\n\n$$\\mathrm{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^M e^{z_j}} \\;\\;\\; \\mathrm{for} \\,\\, i=1,\\ldots,M $$\n\nwhere $z_i$ denotes the nodes of the output layer. Since the outputs from this activation function always sum up to 1, we can interpret them as probabilities of the input belonging to the different classes.\n\nCommonly, the label with maximum probability $p_i$ is chosen as the prediction $y$:\n\n$$ y = \\mathop{\\arg\\max}\\limits_{i} (p_1,\\ldots,p_M). $$"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "210892e9", "tags": []}, "outputs": [], "source": "# Numpy example code for the Softmax function (1D vector)\ndef softmax(x):\n    sum_exps = np.sum(np.exp(x))\n    out = np.exp(x) / sum_exps\n    # or: out = np.exp(x - np.log(sum_exps))\n    return out"}, {"cell_type": "markdown", "metadata": {"id": "vqCgyiJyxoiO", "tags": []}, "source": "---\n##  2. Exploring basic Neural Network structures\nUsing the [NN playground](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.24826&showTestData=false&discretize=true&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&showTestData_hide=false) is a good way to get familiar with how a neural network works. With it you are able to design a simple neural network and train it directly in your browser.\n\nThe training of a neural network is performed with a method called stochastic gradient descent, which you will learn more about in the next lecture. The weights and biases in the network are initialized with random weights and then will be updated iteratively in order to minimize a certain loss function. The training only finds a local minimum of the loss function so different parameter initializations can give different results.\n\nIn the following tasks you will design and train a neural network for a simple classification problem.\n\n**Tasks:**\n1. Open the link above (settings will be pre-selected).\n\n2. Remove both the hidden layers and click the play button to train your network.\n\n3. Select $X_1^2$ and $X_2^2$ as the current input features to train your model. As you can see this gives a very good fit to the provided data (for most of the parameter initializations). Why is that?\n\n4. Go back to using $X_1$ and $X_2$ as input features and add a hidden layer with 6 neurons. Train your model.\n\n5. Try different numbers of neurons in the hidden layer. What is the minimum number of neurons that the model needs to obtain a good fit to this data?\n\n"}, {"cell_type": "markdown", "metadata": {"id": "JLHYyu3_ALp6", "tags": []}, "source": "---\n## 3. Deep Neural Network Implementation [*optional*]\n\nThis is an **optional** section to show a simple implementation of deep neural networks with modern automatic differentiation libraries such as [PyTorch](https://pytorch.org/). Based on the these libraries, we only need a few steps to design the full model:\n\n1. Prepare datasets\n2. Define the network\n3. Train the network\n4. Prediction\n\nIn Appendix, we provide an example code for DNN-based binary classification. You can run the code and take a look at the plot of the dataset."}, {"cell_type": "markdown", "metadata": {"id": "aabf86fc", "tags": []}, "source": "---\n\n# Take-home message\n\n* Activation functions are used to insert non-linearity into a neural network.\n  * **ReLU**, **Sigmoid** and **Tanh** are commonly used as activation functions for the hidden layers.\n  * The **logistic function** is used on the output layer in a binary classifier.\n  * The **softmax** function is used on the output layer for multiclass classification.\n* Different parameter initializations can give different results from the training of a neural network.\n* The model complexity of a neural network is increased with the number of neurons and the number of hidden layers.\n* It is relatively simple to implement a neural network with libraries such as PyTorch.\n\n**Recommendation for further reading:** The material covered in this notebook is well-covered in Chapter 6.1 and Chapter 6.2 of the course book *Machine Learning - A First Course for Engineers and Scientists*."}, {"cell_type": "markdown", "metadata": {"id": "rZ3q9FhcALp8", "tags": []}, "source": "---\n\n# Appendix<br> <sub>Implementing a simple Neural Network in PyTorch</sub>\nThis section is included to give you a sense of what is needed to implement your own neural network [*optional*]."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "HleNfBESALp8", "tags": []}, "outputs": [], "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader"}, {"cell_type": "markdown", "metadata": {"id": "M8JB-mA6ALp9", "tags": []}, "source": "### 1. Prepare datasets"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "TF8nthusALp9", "tags": []}, "outputs": [], "source": "import pandas as pd\nfrom sklearn.datasets import make_moons\n\n# generate 2d classification dataset\nX, y = make_moons(n_samples=500, noise=0.1, random_state=111)\n\n# generate the validation dataset\nX_valid, y_valid = make_moons(n_samples=500, noise=0.1, random_state=444)\n\n# convert all numpy data to PyTorch tensors\nX_train_tensor = torch.tensor(X).float()\ny_train_tensor = torch.tensor(y).float()\nX_valid_tensor = torch.tensor(X_valid).float()\ny_valid_tensor = torch.tensor(y_valid).float()\n\n# scatter plot, dots colored by class value\ndf = pd.DataFrame(dict(x1=X[:,0], x2=X[:,1], label=y))\ncolors = {0:'red', 1:'blue'}\nfig, ax = plt.subplots()\ngrouped = df.groupby('label')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter', x='x1', y='x2', label=key, color=colors[key])\nplt.show()"}, {"cell_type": "markdown", "metadata": {"id": "nIkq4ml5ALp-", "tags": []}, "source": "### 2. Define the network.\nIn this case we use one hidden layer with 16 neurons."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "K5v8UcmdALp-", "tags": []}, "outputs": [], "source": "class BinaryClassifier(nn.Module):\n    def __init__(self, channel_in=2, hidden_dim=16):\n        super().__init__()\n        self.fc1 = nn.Linear(channel_in, hidden_dim) # hidden layer with 16 nodes\n        self.act = nn.ReLU() # activation function\n        self.fc2 = nn.Linear(hidden_dim, 1) # output layer\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n\n# define the network specified above\nmodel = BinaryClassifier()"}, {"cell_type": "markdown", "metadata": {"id": "RUhCp9jAALp-", "tags": []}, "source": "### 3. Train the network\nIn the code cell below, the training of the neural network is performed. Run the cell and inspect the training loss curve."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "lqaknHisALp_", "tags": []}, "outputs": [], "source": "def train(dataloader, model, optimizer, num_epochs=10):\n    losses = []\n    for epoch in range(num_epochs):\n        for inputs, labels in dataloader:\n            preds = model(inputs).squeeze()\n            # compute loss\n            loss = F.binary_cross_entropy(preds, labels)\n            # optimize parameters\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        losses.append(loss.item()) # we use .item() to convert a tensor to numpy\n        #print(f'Epoch {epoch}/{num_epochs}\\tloss: {loss.item():.4f}')\n    return losses\n\ndef predict_accuracy(model, inputs, labels):\n    probs = model(inputs).squeeze()\n    preds = (probs > 0.5).float()\n    accuracy = (preds == labels).float().mean()\n    return accuracy.item()\n\ndef plot_loss(losses):\n    plt.plot(losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training loss curve')\n\n# hyperparameters for the training\nbatch_size = 32\nlearning_rate = 1\nnum_epochs = 100\n\n# prepare the datasets\ndataset = TensorDataset(X_train_tensor, y_train_tensor)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# train the network using stochastic gradient descent\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\nlosses = train(dataloader, model, optimizer, num_epochs)\nplot_loss(losses)"}, {"cell_type": "markdown", "metadata": {"id": "lnTa-lThALp_", "tags": []}, "source": "### 4. Prediction\nTo finish up let's take a look at the decision boundary of our trained model. Run the code below and inspect the result."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "F7SArZbNALqA", "tags": []}, "outputs": [], "source": "# prediction\ntrain_accuracy = predict_accuracy(model, X_train_tensor, y_train_tensor)\nvalid_accuracy = predict_accuracy(model, X_valid_tensor, y_valid_tensor)\nprint(f'Training accuracy: {train_accuracy:.4f}, validation accuracy: {valid_accuracy:.4f}')\n\n#This code cell plots the decision boundary together with validation data\n\nfig, ax = plt.subplots()\nax.set_xlim((-1.5, 2.5))\nax.set_ylim((-0.75, 1.25))\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\n#classify a grid of points to visualize the decision boundary\nres = 0.01\nxs1 = np.arange(-1.5, 2.5, res)\nxs2 = np.arange(-0.75, 1.25, res)\nx1, x2 = np.meshgrid(xs1, xs2)\nX_all = np.column_stack([x1.flatten(),x2.flatten()])\nresult = model(torch.tensor(X_all).float())\nresult = (result > 0.5).float()\ncolors = np.where(result==0,'lightsalmon', 'skyblue').flatten()\nax.scatter(x1.flatten(), x2.flatten(), s = 10, marker='s', c=colors)\n\n# plot validation data\nax.plot(X_valid[:,0][y_valid==1], X_valid[:,1][y_valid==1], 'o', color='b')\nax.plot(X_valid[:,0][y_valid==0], X_valid[:,1][y_valid==0], 'o', color='r')\nplt.show()"}], "metadata": {"celltoolbar": "Tags", "colab": {"collapsed_sections": ["rZ3q9FhcALp8"], "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 5}